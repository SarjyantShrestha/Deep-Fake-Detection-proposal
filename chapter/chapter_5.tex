    \chapter{Methodology}
       \section{Software development approach}
        Agile development is a software development approach that emphasizes incremental progress and rapid cycles. It involves releasing small increments of functionality that build upon previous versions. Thorough testing is conducted for each release to ensure software quality. Agile is often employed for time-critical applications. Although this project is not time-critical this model seems to be the most optimal and practical in our case.
        \begin{figure}[hbt!]
            \center{
                \includegraphics[width=0.75\textwidth]{./img/agile.png}
                \caption{Agile Model for Software Development}
                \subcaption*{\textit{source: \textcolor{blue}{https://mobilelive.medium.com/agile-development-a-comprehensive-guide-for-the-modern-era-d2fe9ae7b395}}}
            }
        \end{figure}

        \section{Implementation}

		\subsection{CNN}
            A Convolutional Neural Network (CNN) \cite{oshea2015introduction} is a Deep Learning algorithm
            that can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and be able to differentiate one from the other. The pre-processing required in a CNN is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, CNNs have the ability to learn these filters characteristics.The architecture of a CNN is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area.A CNN typically has three layers: a convolutional layer, a pooling layer, and a fully connected layer.

        \begin{figure}[hbt!]
                \center{
                    \includegraphics[width=0.85\textwidth]{./img/CNN.png}
                }
                \caption{Convolutional Neural Networks} \cite{E1ICAW_2018_v16n3_173}
        \end{figure}

        \subsection{ResNet}
            ResNet\cite{DBLP:journals/corr/HeZRS15} architecture introduced the concept called Residual Blocks. In this network, we use a technique called skip connections. The skip connection connects activations of a  layer to further layers by skipping some layers in between. This forms a residual block. Resnets are made by stacking these residual blocks together. The approach behind this network is instead of layers learning the underlying mapping, we allow the network to fit the residual mapping. So, instead of say H(x), initial mapping, let the network fit,
            \begin{equation}
                F(x) := H(x) - x
            \end{equation}
            which gives
            \begin{equation}
                H(x) := F(x) + x
            \end{equation}
            \begin{figure}[hbt!]
                \center{
                \includegraphics[width=0.9\textwidth]{./img/ResNet.PNG}
                }
                \caption{ResNet} \cite{enwiki:1205293224}
            \end{figure}

		\pagebreak
		\subsection{Data Preprocessing}
        \subsubsection{Data Collection}
        We used dataset created during a Deepfake Image Detection and Reconstruction Challenge.Two datasets of real face images were used: CelebA and FFHQ. Various Deepfake images were generated using architectures such as StarGAN, GDWCT, AttGAN, StyleGAN, and StyleGAN2. Specifically, CelebA images were manipulated using pre-trained models available on GitHub for StarGAN, GDWCT, and AttGAN. Images from StyleGAN and StyleGAN2 created through FFHQ were obtained.
        
        \begin{enumerate}
            \item CelebA\cite{7410782}: A large-scale face attributes dataset containing over 200k celebrity images with 40 labels related to facial attributes such as hair color, gender, and age. The images are in 178 x 218 JPEG format.
            
            \item FFHQ\cite{NVlabs_ffhq_dataset}: A high-quality image dataset of human faces with variations in age, ethnicity, and image background. The images are in 1024 x 1024 PNG format.
            
            \item StarGAN\cite{choi2018stargan}: Capable of performing image-to-image translations on multiple domains using a single model. CelebA images were manipulated with a pre-trained model to achieve a final resolution of 256 x 256.
            
            \item GDWCT\cite{cho2019imagetoimage}: Improves styling capability. CelebA images were manipulated with a pre-trained model to achieve a final resolution of 216 x 216.
    
            \item AttGAN\cite{8718508}: Transfers facial attributes with constraints. CelebA images were manipulated with a pre-trained model to achieve a final resolution of 256 x 256.
        
            \item StyleGAN\cite{Karras_2020_CVPR}: Transfers semantic content from a source domain to a target domain with a different style. Images were generated using FFHQ as the input dataset with a resolution of 1024 x 1024.
        
            \item StyleGAN2\cite{inproceedings}: Improves StyleGAN quality with the same task. Images were generated using FFHQ as the input dataset with a resolution of 1024x1024.
        \end{enumerate}
        
        \begin{figure}[hbt!]
            \centering
            \begin{minipage}{0.45\textwidth}
                    \centering
                    \includegraphics[width=0.95\linewidth]{./img/real sample.png}
                    \caption{Real Images}
            \end{minipage}
            \hfill
            \begin{minipage}{0.45\textwidth}
                    \centering
                    \includegraphics[width=0.95\linewidth]{./img/fake sample.png}
                    \caption{Fake Images}
            \end{minipage}
        \end{figure}

    \subsubsection{Data Augmentation}
    To balance our dataset, we augmented our images to achieve a total of 25000 images for each category. For the 5000 fake images, we applied four different transformations, resulting in 20000 augmented fake images. For the real images, we randomly selected and transformed 3750 images, generating 15000 augmented real images. \\\\
    The transformations applied were as follows:
    \begin{enumerate}
        \item Rotation: Images were rotated at an angle randomly selected from [45, 90, 135, 180, 225, 270, 315] degrees.
        \item Mirror: Images were randomly flipped horizontally, vertically, or both.
        \item Scale: Images were randomly scaled at a ratio between [0.5,2].
        \item Compression: Images were compressed using a quality factor between [50,99].
    \end{enumerate}
    After adding the augmented images to the original dataset, we ended up with a total of 25000 real and 25000 fake images.

	\begin{figure}[hbt!]
		\centering
		\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=0.95\linewidth]{./img/rotated.png}
				\caption{Rotated Images}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=0.95\linewidth]{./img/compressed.png}
				\caption{Compressed Images}
		\end{minipage}

		\vspace{0.5cm} % Add some vertical space between the two rows of images

		\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=0.95\linewidth]{./img/scaled.png}
				\caption{Scaled Image}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
				\centering
				\includegraphics[width=0.95\linewidth]{./img/mirrored.png}
				\caption{Mirrored Images}
		\end{minipage}
	\end{figure}
	\pagebreak
	\subsubsection{Data Normalization}
	First, we computed the mean and standard deviation of the entire dataset, which consists of both the original dataset and the augmented dataset. The normalization process was then applied using the following formula:
	\begin{equation}
	x = \frac{x - \mu}{\sigma}
	\end{equation}
	where \(x\) represents the pixel values of each image pixel. \\\\
	This approach standardizes the features to have a mean of 0 and a standard deviation of 1. This standardization is crucial because certain machine learning algorithms are sensitive to the scale of the input features.

	\pagebreak
    \subsubsection{Data Splitting}
        We partitioned our dataset into a training set, comprising 80\% of the dataset, and a validation set, consisting of 20\% of the dataset. This division ensures that the model has an ample amount of data for learning, while also offering a subset of the data to assess the model's performance.

	\subsection{Setting Parameters}
	\begin{itemize}
		\item \textbf{Number of epochs : 50} \\
			The number of epochs refers to the number of times the complete training dataset is passed through the network during the training process. Each epoch consists of one forward pass (input to output) and one backward pass (error calculation and weight updates) for all the training samples.
		\item \textbf{Loss function : Cross-Entropy}\cite{mao2023crossentropy} \\
			Loss function is a method of evaluating how well your algorithm models your dataset. Cross-entropy loss measures the difference between a deep learning classification model's discovered and predicted probability distributions.

			The cross-entropy between two probability distributions, such as q from p, can be stated formally as
			
			\begin{equation}
				H(p, q) = -\sum_{x \in \mathcal{X}} p(x) \log q(x)
			\end{equation}

			Where
			\begin{itemize}
				\item H is the cross-entropy function
				\item p may be the target distribution
				\item q is the approximation of the target distribution.
			\end{itemize} 

		\item \textbf{Learning rate : 0.001} \\
			The learning rate is a hyperparameter that determines the step size at which an optimization algorithm.
		\item \textbf{Optimizer : Adam} \cite{kingma2017adam}\\
			An optimizer  is an algorithm used to update the parameters (weights and biases) of a model during training in order to minimize the loss function. 
			Adam (short for Adaptive Moment Estimation) is a popular optimization algorithm known for its robustness, efficiency, and ease of use. It often converges faster and performs better than traditional optimization algorithms.
			It adapts the learning rate for each parameter individually based on the past gradients and squared gradients making it well suited for training models.
	\end{itemize}
\pagebreak
	\subsection{Model Training}
		We tested various customs models as well as pre-trained models like VGG16\_bn\cite{simonyan2015deep}, ResNet50 \cite{DBLP:journals/corr/HeZRS15}, ResNet101\cite{DBLP:journals/corr/HeZRS15}, and many more. During this process, We came to a conclusion that ResNet9 was performing much better than other models. So, We are using ResNet9 architecture for further training and implementation process.\\\\
		The model begins with an input layer configured to accept images with dimensions of 128x128 pixels and three color channels (RGB). Following this, the first convolutional layer is applied, utilizing 64 filters of size 3x3. Each filter convolves across the input image, extracting various features such as edges and textures. Batch normalization is then performed to normalize the activations of the convolutional layer, enhancing training stability. Subsequently, a rectified linear unit (ReLU) activation function is applied, introducing non-linearity to the network by replacing negative values with zeros.\\\\
		Moving forward, the second convolutional layer processes the output of the previous layer, applying 128 filters of size 3x3. Again, batch normalization and ReLU activation follow suit. Additionally, a max-pooling layer with a 2x2 window and a stride of 2 downsamples the feature maps, reducing spatial dimensions and computational complexity.\\\\
		Next, a residual block, inspired by the ResNet architecture, is introduced. This block comprises two convolutional layers, each followed by batch normalization and ReLU activation. The output of these layers is added to the output of the second convolutional layer, fostering better gradient flow during training and mitigating the vanishing gradient problem.\\\\
		Continuing, convolutional layer 3 is applied, employing 256 filters of size 3x3. Batch normalization and ReLU activation are again utilized for feature map normalization and non-linearity introduction, respectively. Another max-pooling layer further reduces spatial dimensions.\\\\
		Subsequently, convolutional layer 4 is employed, employing 512 filters of size 3x3. Similar to prior layers, batch normalization and ReLU activation are applied. Following this, another residual block, akin to ResNet architecture, is employed, aiding in feature extraction and gradient flow enhancement.\\\\
		Finally, a max-pooling layer with a 4x4 window reduces the spatial dimensions further. The resultant feature maps are then flattened into a 1D vector, which undergoes dropout regularization with a rate of 0.2, randomly deactivating 20\% of neurons during training to prevent overfitting. Lastly, a fully connected layer with two units, likely indicative of binary classification, applies a softmax activation function to generate class probabilities.
	\newpage
	\begin{figure}[hbt!]
		\center{
			\includegraphics[width=1\textwidth]{./img/layers.png}
			\caption{Architecture of ResNet9 Model}
		}
	\end{figure}

	\clearpage
	\begin{table}
		\begin{tabular}{|l|l|l|l|}
		\hline
		\textbf{Layer} & \textbf{Layer Type} & \textbf{Output Dimension} & \textbf{No. of Parameters} \\ \hline
		1 & Input (Convolution 2-D) & (64*128*128) & 1792 \\ \hline
		2 & Batch Normalization & (64*128*128) & 256 \\      \hline
		3 & ReLU & (64*128*128) & 0 \\                       \hline
		4 & Convolution 2-D & (128*128*128) & 73,856 \\        \hline
		5 & Batch Normalization & (128*128*128) & 512 \\     \hline
		6 & ReLU & (128*128*128) & 0 \\                      \hline
		7 & MaxPooling & (128*64*64) & 0 \\                  \hline
		8 & Convolution 2-D & (128*64*64) & 147,584 \\          \hline
		9 & Batch Normalization & (128*64*64) & 512 \\       \hline
		10 & ReLU & (128*64*64) & 0 \\                       \hline
		11 & Convolution 2-D & (128*64*64) & 147,584 \\         \hline
		12 & Batch Normalization & (128*64*64) & 512 \\      \hline
		13 & ReLU & (128*64*64) & 0 \\                       \hline
		14 & Convolution 2-D & (256*64*64) & 295,168 \\         \hline
		15 & Batch Normalization & (256*64*64) & 1024 \\     \hline
		16 & ReLU & (128*64*64) & 0 \\                       \hline
		17 & MaxPooling & (256*32*32) & 0 \\                 \hline
		18 & Convolution 2-D & (512*32*32) & 1,180,160 \\        \hline
		19 & Batch Normalization & (512*32*32) & 2048 \\     \hline
		20 & ReLU & (512*32*32) & 0 \\                       \hline
		21 & MaxPooling & (512*16*16) & 0 \\                 \hline
		22 & Convolution 2-D & (512*16*16) & 2,359,808 \\        \hline
		23 & Batch Normalization & (512*16*16) & 2048 \\     \hline
		24 & ReLU & (512*16*16) & 0 \\                       \hline
		25 & Convolution 2-D & (512*16*16) & 2,359,808 \\        \hline
		26 & Batch Normalization & (512*16*16) & 2048 \\     \hline
		27 & ReLU & (512*16*16) & 0 \\                       \hline
		28 & MaxPooling & (512*4*4) & 0 \\                   \hline
		29 & Flatten & 8192 & 0 \\                           \hline
		30 & Dropout & 8192 & 0 \\                           \hline
		31 & Dense & 2 & 16386 \\\hline
		\end{tabular}
	\end{table}

	\begin{table}[hbt!]
		\begin{tabularx}{1.03\textwidth}{|X|X|}
			\hline
			Total Parameters & 6,591,106 \\ \hline
			Trainable Parameters & 6,586,626 \\  \hline
			Non-Trainable Parameters & 4,480 \\ \hline
		\end{tabularx}
		\caption{Output Dimensions and Parameter for Each Layer}
	\end{table}
        
        
        
        
        % \subsection{GanttChart}
        %     \begin{figure}[hbt!]
        %         \center{
        %             \includegraphics[width=0.1\textwidth]{./img/GANT_CHART.jpg}
        %         }
        %         \caption{Gantt chart}
        %     \end{figure}
